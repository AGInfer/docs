---
title: "Docker"
description: "Bring your own docker container images. ( This might have higher coldstarts )"
---
<Note>
This is depreciated in the new version of Inferless, If you need to import via docker please reach out to us at alerts@inferless.com
</Note>

You can provide a custom docker image to host your model on the inferless platform. Supported formats for custom docker images.

* Private Image URI
* Dockerfile

For Inferless to work with the Image you have to meet the following specification :

- **Health check API Path** (GET) - An endpoint that monitors the application's status, ensuring it's running and operational. For the below code, you need to give “/health/live” as the path

```json
@app.get("/health/live")
def health_check():
    return {"status": "running"}
```

- **Infer API** (POST) - An endpoint that processes input data (e.g., text or images) and returns results generated by a model, such as predictions or outputs. ( `--inferapi` )

```json
@app.post("/infer")
def generate_image(request: InferRequest):
    return {"image": img_str.decode('utf-8')}  # Return Base64 string
```

- **Server Port** - The port on which the model server is running ( —port )

```docker
#Dockerfile 

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

- **Input (optional)** - Sample JSON input for model inference you can pass this a raw data( —data)  or path the file ( —datapath )

```json
{
  "text" : "a horse near a lake"
}
```

### Example
<Card icon = "github" href="https://github.com/inferless/inferless-docker-import-examples">
GitHub - inferless/inferless-docker-import-examplesGitHub
</Card>

## UI walkthrough for Docker Hub Import 

### Step 1: Add Model in your workspace.

* Navigate to your desired workspace in Inferless and Click on `"Add a custom model" `button that you see on the top right. An import wizard will open up.

<Frame caption = "Click on Add Model">![](/images/step1-common-add-model.png)</Frame>

###  Step 2: Choose the source of your model.

* Since we are using a model from Docker Hub, select `Docker Hub`as the method of upload from the `Provider` list.

* To proceed with the upload, you will need to connect your `Docker Hub account`. This is a mandatory step as this helps us download the image from your Docker Hub.

<Frame caption = "After you have selected Repo ">![](/images/doc1-provider.png)</Frame>

### Step 3: Enter the model details 
* Provide the details of `Image URL`, `Health check API`, `Infer API` and `Server Port`
* `Model Name`: The desired name of the model that you wish to give

<Frame caption = "Enter the details as noted">![](/images/doc2-model-details.png)</Frame>
   * In case you would like to set up `Automatic rebuild` for your model, enable it  
         * You would need to set up a webhook for this method. Click [here](/model-import/automatic-build-via-webhooks) for more details.


### Step 4: Configure Machine and Environment.
* Choose the type of machine, and specify the minimum and maximum number of replicas for deploying your model.
         * **Min scale** \-  
         ```  
         The number of inference workers to keep on at all times.  
         ```  
         * **Max scale -**  
         ```  
         The maximum number of inference workers to allow at any point of time  
         ``` 
         * Configure Custom Runtime ( If you have pip or apt packages), choose Volume, Secrets and set Environment variables like Inference Timeout / Container Concurrency / Scale Down Timeout

<Frame caption = "Set runtime and configuration">![](/images/common-machine-env.png)</Frame>


### Step 5: Review your model details

* Once you click "Continue," you will be able to review the details added for the model.

* If you would like to make any changes, you can go back and make the changes.

* Once you have reviewed everything, click `Deploy` to start the model import process.

<Frame caption = "Review all the details carefully before proceeding">![](/images/common-deploy.png)</Frame>


### Step 6 : Run your model

* Once you click submit, the model import process would start.  
* It may take some time to complete the import process, and during this time, you will be redirected to your workspace and can see the status of the import under `"In Progress/Failed"` tab.  
<Frame caption = "View the model under `In-Progress/ Failed`">![](/images/common-in-progress.png)</Frame>
  
* If you encounter any errors during the model import process or if you want to view the build logs for any reason, you can click on the three dots menu and select "View build logs". This will show you a detailed log of the import process, which can help you troubleshoot any issues you may encounter.  
* Post-upload, the model will be available under "My Models"  
* You can then select the model and go to `-> API -> Inference Endpoint details. `Here you would find the API endpoints that can be called. You can click on the copy button on the right and can call your model.  
  
<Frame caption = "Under the API Tab, you can view the API endpoint details."> ![](/images/common-api-call.png)  </Frame>

### Extra Step: Getting API key details



* You can now call using this from your end. The inference result would be the output for these calls.  
* In case you need help with API Keys:  
   * Click on settings, available on the top, next to your Workspace Name  
   * Click on "Workspace API keys"  
   * You can view the details of your key or generate a new one  
<Frame caption = "Sample for now">![](/images/common-workspace-api.png)</Frame>

### Using CLI 

You can deploy the model with Inferless using docker image. Just make sure the image is stored either in dockerhub/ ecr

1. Connect your [Dockerhub](https://hub.docker.com/) account using this command(below) and enter the username and access token
    
    ```bash
    inferless integration add dockerhub --name <integrationname> --username <name> --access-token <token>
    ```
    
2. Once you’re done with integration. Run this command to initialise the model
    
    ```bash
    inferless init-docker --type dockerimage --name <modelname> --provider <dockerhub/ecr> --url <imageurl> --healthapi <apiname> --inferapi <apiname> --serverport <portnumber> --autobuild 
    ```
    
3. Now that your model is initialised. 
    - To do default deployment use this
    
    ```bash
    inferless deploy --gpu t4 
    ```
    
    - To do customised deployment use this
    
    ```bash
    inferless deploy --gpu t4 --region <regionname> --runtime <runtimename> --volume <volumename> --fractional    
    ```

## UI walkthrough for Dockerfile Import 

### Step 1: Add Model in your workspace.

* Navigate to your desired workspace in Inferless and Click on `"Add a custom model" `button that you see on the top right. An import wizard will open up.

<Frame caption = "Click on Add Model">![](/images/step1-common-add-model.png)</Frame>

###  Step 2: Choose the source of your model.

* Since we are using a model from Dockerfile, select `Dockerfile`as the method of upload from the `Provider` list.

* To proceed with the upload, you will need to connect your `GitHub/GitLab account`. This is a mandatory step as this helps us get the file from your repository.

<Frame caption = "After you have selected Repo ">![](/images/docf-1-provider.png)</Frame>

### Step 3: Enter the model details 
* Select your `Github Repository` and the `branch`.
* Provide the details of `Health check API`, `Infer API`, `Server Port` and the `Docker File Path`
* `Model Name`: The desired name of the model that you wish to give

<Frame caption = "Enter the details as noted">![](/images/docf-2-model-details.png)</Frame>
   * In case you would like to set up `Automatic rebuild` for your model, enable it  
         * You would need to set up a webhook for this method. Click [here](/model-import/automatic-build-via-webhooks) for more details.


### Step 4: Configure Machine and Environment.
* Choose the type of machine, and specify the minimum and maximum number of replicas for deploying your model.
         * **Min scale** \-  
         ```  
         The number of inference workers to keep on at all times.  
         ```  
         * **Max scale -**  
         ```  
         The maximum number of inference workers to allow at any point of time  
         ``` 
         * Configure Custom Runtime ( If you have pip or apt packages), choose Volume, Secrets and set Environment variables like Inference Timeout / Container Concurrency / Scale Down Timeout

<Frame caption = "Set runtime and configuration">![](/images/common-machine-env.png)</Frame>


### Step 5: Review your model details

* Once you click "Continue," you will be able to review the details added for the model.

* If you would like to make any changes, you can go back and make the changes.

* Once you have reviewed everything, click `Deploy` to start the model import process.

<Frame caption = "Review all the details carefully before proceeding">![](/images/common-deploy.png)</Frame>


### Step 6 : Run your model

* Once you click submit, the model import process would start.  
   * It may take some time to complete the import process, and during this time, you will be redirected to your workspace and can see the status of the import under `"In Progress/Failed"` tab.  
<Frame caption = "View the model under `In-Progress/ Failed`">![](/images/common-in-progress.png)</Frame>
  
   * If you encounter any errors during the model import process or if you want to view the build logs for any reason, you can click on the three dots menu and select "View build logs". This will show you a detailed log of the import process, which can help you troubleshoot any issues you may encounter.  
   * Post-upload, the model will be available under "My Models"  
   * You can then select the model and go to `-> API -> Inference Endpoint details. `Here you would find the API endpoints that can be called. You can click on the copy button on the right and can call your model.  
  
<Frame caption = "Under the API Tab, you can view the API endpoint details."> ![](/images/common-api-call.png)  </Frame>

### Extra Step: Getting API key details



* You can now call using this from your end. The inference result would be the output for these calls.  
   * In case you need help with API Keys:  
   * Click on settings, available on the top, next to your Workspace Name  
   * Click on "Workspace API keys"  
   * You can view the details of your key or generate a new one  
<Frame caption = "Sample for now">![](/images/common-workspace-api.png)</Frame> 

### Using CLI

If you’ve a dockerfile with you, we can build the image. Just make sure you’re dockerfile is stored in Github/ Gitlab for us to access it.

1. Login to [Inferless.com](https://inferless.com/) and click on three dot menu near the profile picture
2. Click on [Manage integrations](https://console.inferless.com/user/integration) and connect with your Github/ Gitlab account were the model is stored.
3. Once you’re done with integration on console. Open CLI and run this command to initialise the model
    
    ```bash
    inferless init docker --type dockerfile --name <modelname> --provider <github/gitlab> --url <repoURL> --healthapi <apiname> --inferapi <apiname> --dockerfilepath <path> --serverport <portnumber> --autobuild 
    ```
    
4. Now that your model is initialised. 
    - To do default deployment use this
    
    ```bash
    inferless deploy --gpu t4 
    ```
    
    - To do customised deployment use this
    
    ```bash
    inferless deploy --gpu t4 --region <regionname> --runtime <runtimename> --volume <volumename> --fractional
    ```
