---
title: "Configuring Concurrent Requests"
description: "This will help you understand how to process multiple requests concurrently by the same replica."
---

Inferless allows you to process multiple requests concurrently by the same replica. This can help you improve the throughput of your model and handle multiple requests simultaneously. In this guide, we'll walk you through the steps to configure your model to handle concurrent requests.

There are 2 ways to configure concurrent requests in Inferless:

* ** Sequentical Processing with Queue**

* ** Batch Processing with Queue **


## Sequential Processing with Queue

This is the simplest way to process multiple requests with the same replica. In this method, the requests are processed sequentially by the same replica. This is useful when you have task that takes lesser time to process. 

To configure this you can go to Model Import -> Settings

![](/images/container-concurrency.png)

Set the Container Concurrency to 'desired_number' and click on Update. You can set any value between 1 to 100.

## Batch Processing with Queue

In this method, the requests are processed in batches by the same replica. This is useful when you have tasks that take longer time to process and you want to process multiple requests simultaneously.

#### Step 1: Preparing the model to handle concurrent requests

Define the BATCH\_SIZE and BATCH\_WINDOW in the **input\_schema.py**

input\_schema.py 

```input_schema
INPUT_SCHEMA = {
    "prompt": {
        'datatype': 'STRING',
        'required': True,
        'shape': [1],
        'example': ["There is a fine house in the forest"]
    }
}
BATCH_SIZE = 4
BATCH_WINDOW = 5000 # milliseconds
```

in app.py 

```python
import json
import numpy as np
import torch
from transformers import pipeline

class InferlessPythonModel:

    def initialize(self):
        self.generator = pipeline("text-generation", model="EleutherAI/gpt-neo-125M",device=0)

    # Inputs is a list of dictionaries where the keys are input names and values are actual input data
   
    # Output generated by the infer function should be a List of dictionaries where keys are output names and values are actual output data
    def infer(self, inputs):
        output = []

        print(" no of inputs to be processed " + str(len(inputs)))
        for each in inputs:
            prompt = each["prompt"]
            pipeline_output = self.generator(prompt, do_sample=True, min_length=20)
            generated_txt = pipeline_output[0]["generated_text"]
            output.append({"generated_text": generated_txt })
        return output

    # perform any cleanup activity here
    def finalize(self,args):
        self.pipe = None
```

More on batching can be found [here](/concepts/dynamic-batching)

#### Step 2: Configuring the model to handle concurrent requests

Go to Model Import -> Settings

![](/images/container-concurrency.png)

Set the Container Concurrency to 'desired_number' if.e 4 and click on Update. You can set any value between 1 to 100.



