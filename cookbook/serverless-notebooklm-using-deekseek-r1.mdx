---
title: "Build a Serverless NotebookLM using DeepSeek-R1-Distill Model and Kokoro-TTS"
description: "Welcome to an engaging tutorial designed to walk you through creating a open-source NotebookLM. You'll learn to integrate reasoning model like DeepSeek-R1-Distill and Kokoro text-to-speech model to develop a interesting podcast 
from a PDF file."
---
## Key Components of the Application
In building this application, we'll utilize these components:

1. __Reasoning Model:__ We will use the latest [DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) to help creating the script from the PDF file.
2. __Text-to-Speech:__ We will use [Kokoro-TTS](https://huggingface.co/hexgrad/Kokoro-82M) which will convert the text to interesting speech conversation.

## Crafting Your Application
This tutorial guides you through creating a NotebookLM like application where users can provide a PDF file and it will return you a quick summarization in a podcast format. 
It leverages technologies such as [Transformers](https://github.com/huggingface/transformers) and [Kokoro-TTS](https://github.com/hexgrad/kokoro).


## Core Development Steps
### Text Conversation Generation
- __Objective:__ Capture user provided pdf and convert it into a conversational text between two person.
- __Action:__ Implement a Python class ([InferlessPythonModel](https://github.com/inferless/Customer-Service-Voicebot/blob/main/app.py)) to handle the entire process, including input handling, model integration, and conversation generation.

```python

```
### Setting up the Environment
__Dependencies:__
- __Objective:__ Ensure all necessary libraries are installed.
- __Action:__ Run the command below to install dependencies:
```bash
pip install transformers==4.46.1 pypdf==5.1.0 spacy==3.7.5 kokoro==0.3.1 soundfile==0.12.1 accelerate==1.0.1 pydub==0.25.1
```
This command ensures your environment has all the tools required for the application.

### Deploying Your Model with Inferless CLI
Inferless allows you to deploy your model using Inferless-CLI. Follow the steps to deploy using Inferless CLI.

#### Clone the repository of the model
Let's begin by cloning the model repository:
```bash
git clone https://github.com/inferless/Customer-Service-Voicebot.git
```

#### Deploy the Model
To deploy the model using Inferless CLI, execute the following command:
```bash
inferless deploy --gpu A100 --runtime inferless-runtime-config.yaml
```

**Explanation of the Command:**

- `--gpu A100`: Specifies the GPU type for deployment. Available options include `A10`, `A100`, and `T4`.
- `--runtime inferless-runtime-config.yaml`: Defines the runtime configuration file. If not specified, the default Inferless runtime is used.

### Demo of the Customer Service Voicebot.
<video width="640" height="360" controls>
  <source src="/videos/customer-support-bot.mp4" type="video/mp4"/>
  Your browser does not support the video tag.
</video>


### Alternative Deployment Method
Inferless also supports a user-friendly UI for model deployment, catering to users at all skill levels. Refer to Inferless's documentation for guidance on UI-based deployment.
## Choosing Inferless for Deployment
Deploying your Customer Service Voicebot application with Inferless offers compelling advantages, making your development journey smoother and more cost-effective. Here's why Inferless is the go-to choice:
1. __Ease of Use:__ Forget the complexities of infrastructure management. With Inferless, you simply bring your model, and within minutes, you have a working endpoint. Deployment is hassle-free, without the need for in-depth knowledge of scaling or infrastructure maintenance.
2. __Cold-start Times:__ Inferless's unique load balancing ensures faster cold-starts. Expect around 2.87 seconds to process each queries, significantly faster than many traditional platforms.
3. __Cost Efficiency:__ Inferless optimizes resource utilization, translating to lower operational costs. Here's a simplified cost comparison:

### Scenario 1
You are looking to deploy a Customer Service Voicebot application for processing 100 queries.<br />

__Parameters:__
- __Total number of queries:__ 100 daily.<br />
- __Inference Time:__ All models are hypothetically deployed on A100 80GB, taking 2.87 seconds of processing time and a cold start overhead of 24.01 seconds.<br />
- __Scale Down Timeout:__ Uniformly 60 seconds across all platforms, except Hugging Face, which requires a minimum of 15 minutes. This is assumed to happen 100 times a day.<br />

__Key Computations:__
1. __Inference Duration:__ <br/>
Processing 100 queries and each takes 2.87 seconds<br/>
Total: 100 x 2.87 = 287 seconds (or approximately 0.08 hours)
2. __Idle Timeout Duration:__<br/>
Post-processing idle time before scaling down: (60 seconds - 2.87 seconds) x 100 = 5713 seconds (or 1.59 hours approximately)<br/>
3. __Cold Start Overhead:__<br/>
Total: 100 x 24.01 = 2401 seconds (or 0.67 hours approximately)<br/>

__Total Billable Hours with Inferless:__ 0.08 (inference duration) + 1.59 (idle time) + 0.67 (cold start overhead)  = 2.34 hours<br/>
__Total Billable Hours with Inferless:__ 2.34 hours<br/>

### Scenario 2
You are looking to deploy a Customer Service Voicebot application for processing 1000 queries per day.<br />

__Key Computations:__<br />
1. __Inference Duration:__<br />
Processing 1000 queries and each takes 2.87 seconds
Total: 1000 x 2.87 = 2870 seconds (or approximately 0.8 hours)‚Äç
2. __Idle Timeout Duration:__<br/>
Post-processing idle time before scaling down: (60 seconds - 2.87 seconds) x 100 = 5713 seconds (or 1.59 hours approximately)<br/>
3. __Cold Start Overhead:__<br/>
Total: 100 x 24.01 = 2401 seconds (or 0.67 hours approximately)<br/>

__Total Billable Hours with Inferless:__ 0.8 (inference duration) + 1.59 (idle time) + 0.67 (cold start overhead)  = 3.06 hours<br/>
__Total Billable Hours with Inferless:__ 3.06 hours<br/>


| Scenarios | On-Demand Cost | Serverless Cost|
| :--- | :---- | :---- |
|  100 requests/day | \$28.8 (24 hours billed at $1.22/hour) | \$2.85 (2.34 hours billed at $1.22/hour) |
|  1000 requests/day | \$28.8 (24 hours billed at $1.22/hour) | \$3.73 (3.06 hours billed at $1.22/hour) |

By opting for Inferless, **_you can achieve up to 90.10% cost savings._**<br/>

Please note that we have utilized the A100(80 GB) GPU for model benchmarking purposes, while for pricing comparison, we referenced the A10G GPU price from both platforms. This is due to the unavailability of the A100 GPU in SageMaker.

Also, the above analysis is based on a smaller-scale scenario for demonstration purposes. Should the scale increase tenfold, traditional cloud services might require maintaining 2-4 GPUs constantly active to manage peak loads efficiently. In contrast, Inferless, with its dynamic scaling capabilities, adeptly adjusts to fluctuating demand without the need for continuously running hardware.<br/>
## Conclusion
By following this guide, you're now equipped to build and deploy a sophisticated Customer Service Voicebot application. This tutorial showcases the seamless integration of advanced technologies, emphasizing the practical application of creating cost-effective solutions.